{
 "metadata": {
  "name": "",
  "signature": "sha256:497001c8df0d23f0775622328b10c567d42e2c51e38df01497a9ced003106035"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GA Data Science 7 (DAT7) - Lab 6.2\n",
      "## Support Vector Machines\n",
      "Modified from the Chris Fonnesbeck git notebook\n",
      "https://github.com/fonnesbeck"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the logistic regression model, which transforms a linear combination of predictors with the logistic function.\n",
      "\n",
      "$$g_{\\theta}(x) = \\frac{1}{1+\\exp(-\\theta\\prime x)}$$\n",
      "\n",
      "Notice that when our response is $y=1$, we want the product $\\theta\\prime x$ to be a very large, positive value so that $g_{\\theta}(x) \\rightarrow 1$, and when $y=0$, we want this product to be a very large, negative value, so that $g_{\\theta}(x) \\rightarrow 0$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pylab as plt\n",
      "%matplotlib inline\n",
      "\n",
      "invlogit = lambda x, theta: 1. / (1. + np.exp(-x.dot(theta)))\n",
      "\n",
      "theta = [2, -0.5]\n",
      "x = np.c_[np.ones(100), np.linspace(-10,20,100)]\n",
      "\n",
      "y = invlogit(x, theta)\n",
      "\n",
      "plt.plot(x.dot(theta), y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The negative log-likelihood (or cost function) for the logistic regression model is as follows:\n",
      "\n",
      "$$l(y_i|\\theta,x) = -[y_i \\log g_{\\theta}(x) + (1-y_i)\\log(1-g_{\\theta}(x))]$$\n",
      "\n",
      "Consider the case where $y_i=1$. This implies that the cost function is:\n",
      "\n",
      "$$l(y_i=1|\\theta,x) = - \\log \\left[ \\frac{1}{1+\\exp(-\\theta\\prime x)} \\right]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x.dot(theta), -np.log(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and when $y_i=0$:\n",
      "\n",
      "$$l(y_i=0|\\theta,x) = - \\log \\left[ 1 - \\frac{1}{1+\\exp(-\\theta\\prime x)} \\right]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x.dot(theta), -np.log(1.-y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way to develop a support vector machine is to modify the logistic regression model by substituting a different cost function, which is just a piecwise linear function.\n",
      "\n",
      "For $y_i=1$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x.dot(theta), -np.log(y))\n",
      "\n",
      "hinge_cost = lambda x, theta: np.maximum(0, 1 - x.dot(theta))\n",
      "\n",
      "plt.plot(x.dot(theta), hinge_cost(x, theta), 'r-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For $y_i=0$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x.dot(theta), -np.log(1-y))\n",
      "\n",
      "hinge_cost = lambda x, theta: np.maximum(0, 1 + x.dot(theta))\n",
      "\n",
      "plt.plot(x.dot(theta), hinge_cost(x, theta), 'r-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now consider the estimation of the parameters of a regularized logistic regression model. This is typically by minimizing:\n",
      "\n",
      "$$\\min_{\\theta} \\frac{1}{n} \\left[ \\sum_{i=1}^n y_i -\\log g_{\\theta}(x_i) + (1-y_i)(-\\log(1-g_{\\theta}(x_i))) \\right] + \\frac{\\lambda}{2n} \\sum_{j=1}^k \\theta^2_j$$\n",
      "\n",
      "for the support vector machine, we instead substitute our cost function (which we will call $k$) in place of the logistic regression likelihood:\n",
      "\n",
      "$$\\min_{\\theta} \\left[ C \\sum_{i=1}^n y_i k_1(\\theta\\prime x_i) + (1-y_i) k_0(\\theta\\prime x_i) \\right] + \\frac{1}{2}\\sum_{j=1}^k \\theta^2_j$$\n",
      "\n",
      "where the parameter $C$ is plays a role equivalent to $1/\\lambda$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that to make these cost functions $k$ small, we want $x \\ge 1$ or $x \\le -1$ rather than just being greater than or less than zero, for $y=1$ or $y=0$, respectively. If we set the parameter $C$ very large, we would want the summation term to be equal or close to zero in order to minimize the overall optimization objective.\n",
      "\n",
      "This objective then essentially becomes:\n",
      "\n",
      "$$\\min_{\\theta} \\frac{1}{2} \\sum_{j=1}^k \\theta^2_j$$\n",
      "$$\\begin{aligned}\n",
      "\\text{subject to }\\theta\\prime x_i \\ge 1 &\\text{ if } y_i=1 \\\\\n",
      "\\theta\\prime x_i \\le -1 &\\text{ if } y_i=0\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a dataset with two linearly separable groups."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g1 = np.random.multivariate_normal((-1,-1), np.eye(2)*0.2, 10)\n",
      "g2 = np.random.multivariate_normal((1,1), np.eye(2)*0.2, 10)\n",
      "\n",
      "plt.scatter(*g1.T, color='r')\n",
      "plt.scatter(*g2.T, color='b')\n",
      "\n",
      "plt.xlim(-2,2); plt.ylim(-2,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One possible separation is a line that passes very close to points in both groups."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x,y = np.transpose([g1[np.where(g1.T[1]==g1.max(0)[1])[0][0]], \n",
      "                       g2[np.where(g2.T[1]==g2.min(0)[1])[0][0]]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*g1.T, color='r')\n",
      "plt.scatter(*g2.T, color='b')\n",
      "\n",
      "x,y = np.transpose([g1[np.where(g1.T[1]==g1.max(0)[1])[0][0]], \n",
      "                       g2[np.where(g2.T[1]==g2.min(0)[1])[0][0]]])\n",
      "b0,b1 = np.linalg.lstsq(np.c_[[1,1],x], y)[0]\n",
      "xspace = np.linspace(-3,3,100)\n",
      "plt.plot(xspace, b0 + (b1-.1)*xspace, 'k--')\n",
      "plt.xlim(-2,2); plt.ylim(-2,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems like a poor choice of decision boundary, even though it separates the groups, because it may not be a *robust* solution. SVM avoids this by establishing a ***margin*** between the decision boundary and the nearest points in each group. This margin is maximized under SVM, and is partly the result of using 1 and -1 as the thresholds for the cost function, rather than zero."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Large-margin Classification\n",
      "\n",
      "To understand how SVM incorporates a margin into its decision boundary, it helps to re-write our objective function in terms of the norm (length) of the parameter vector:\n",
      "\n",
      "$$\\min_{\\theta} \\frac{1}{2} \\sum_{j=1}^k \\theta^2_j = \\min_{\\theta} \\frac{1}{2} ||\\theta||^2$$\n",
      "\n",
      "Recall that when we take the inner product of two vectors, we are essentially projecting the values of one vector onto the other, in order to add them. In the case of our inner product $\\theta\\prime x_i$, we are projecting the ith component of $x$ onto the parameter vector $\\theta$. We can therefore re-write this inner product in terms of multiplying vector lengths:\n",
      "\n",
      "$$\\theta\\prime x_i = p_i ||\\theta||$$\n",
      "\n",
      "where $p_i$ is the projection of $x_i$ onto $\\theta$. The objective function now becomes:\n",
      "\n",
      "$$\\min_{\\theta} \\frac{1}{2} ||\\theta||^2$$\n",
      "$$\\begin{aligned}\n",
      "\\text{subject to }p_i ||\\theta|| \\ge 1 &\\text{ if } y_i=1 \\\\\n",
      "p_i ||\\theta|| \\le -1 &\\text{ if } y_i=0\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "clearly, in order to satisfy this criterion for a given parameter vector $\\theta$, we want the $p_i$ to be *as large as possible*. However, when the decision boundary is close to points in the dataset, the corresponding $p_i$ values will be very small, since they are being projected onto the $\\theta$ vector, which is perpendicular to the decision boundary.\n",
      "\n",
      "Here is a simple graphical illustration of the difference between two boundary choices, in terms of $p_i$ values.\n",
      "\n",
      "First, a boundary choice that passes closely to the points of each class:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frame = plt.gca()\n",
      "frame.axes.yaxis.set_ticklabels([])\n",
      "frame.axes.xaxis.set_ticklabels([])\n",
      "\n",
      "x1 = -1, 0\n",
      "x2 = 1, 1\n",
      "\n",
      "plt.scatter(*x1, s=300, marker='+')\n",
      "plt.scatter(*x2, s=300, marker='+', color='r')\n",
      "plt.plot([-1.5, 1.5],[0,1], 'k-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parameter vector $\\theta$ is perpendicular to this decision boundary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*x1, s=300, marker='+')\n",
      "plt.scatter(*x2, s=300, marker='+', color='r')\n",
      "plt.plot([-1.5, 1.5],[0,1], 'k-')\n",
      "plt.plot([-.5, .75], [1, 0], 'k--')\n",
      "plt.annotate(r\"$\\theta$\", xy=(-0.4, 1), fontsize=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to see what the $p_i$ values will be, we drop perpendicular lines down to the parameter vector $\\theta$. Notice that for this decision boundary, the resulting $p_i$ are quite small (either positive or negative). In order to satisfy our constraint, this will force $||\\theta||$ to be large, which is not desirable given our objective."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*x1, s=300, marker='+')\n",
      "plt.scatter(*x2, s=300, marker='+', color='r')\n",
      "plt.plot([-1.5, 1.5],[0,1], 'k-')\n",
      "plt.plot([-.5, .75], [1, 0], 'k--')\n",
      "plt.annotate(r\"$\\theta$\", xy=(-0.4, 1), fontsize=20)\n",
      "\n",
      "plt.arrow(-1, 0, 3*(.35), .35, fc=\"b\", ec=\"b\", head_width=0.07, head_length=0.2)\n",
      "plt.arrow(1, 1, -3*(.28), -.28, fc=\"r\", ec=\"r\", head_width=0.07, head_length=0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now consider another decision boundary, which intuitively appears to be a better choice."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*x1, s=300, marker='+')\n",
      "plt.scatter(*x2, s=300, marker='+', color='r')\n",
      "plt.plot([-.5,.5], [1.5,-.5], 'k-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can confirm this in terms of our objective function, by showing the corresponding projections $p_i$ to be large, which allows our parameter vector norm to be smaller."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*x1, s=300, marker='+')\n",
      "plt.scatter(*x2, s=300, marker='+', color='r')\n",
      "plt.plot([-.5,.5], [1.5,-.5], 'k-')\n",
      "plt.plot([-1, 1], [-.5, 1.7], 'k--')\n",
      "plt.annotate(r\"$\\theta$\", xy=(0.6, 1.5), fontsize=20)\n",
      "\n",
      "plt.arrow(-1, 0, .1, -.2, fc=\"b\", ec=\"b\", head_width=0.07, head_length=0.1)\n",
      "plt.arrow(1, 1, -.2, .37, fc=\"r\", ec=\"r\", head_width=0.07, head_length=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus, the values of $\\{p_i\\}$ define a *margin* that we are attempting to maximize to aid robust classificaction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Kernels\n",
      "\n",
      "In many instances, we are looking to separate groups that are not separable by a linear boundary.  One way to tackle the problem is to create a higher-order polynomial function to use as a boundary. For example,\n",
      "\n",
      "$$y = \\left\\{ \\begin{aligned} 1 &\\text{if } \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1 x_2 + \\theta_4 x_1^2 + \\ldots \\ge 0 \\\\\n",
      "0 &\\text{ otherwise}\\end{aligned}\\right.$$\n",
      "\n",
      "However, calculating higher-order polynomials can be expensive.\n",
      "\n",
      "An alternative is to employ a function that measures the similarity between two points in the feature space. Generically, such functions are called ***kernels***, and they are characterized by being positive and symmetric, in the sense that for kernel $k$, $k(x,x\\prime) = k(x\\prime, x)$ (Mercer's Theorem). \n",
      "\n",
      "One common kernel is the Gaussian:\n",
      "\n",
      "$$k(x, x\\prime) = \\exp\\left[-\\frac{||x-x\\prime||^2}{2 \\sigma^2}\\right]$$\n",
      "\n",
      "Notice that when $x$ and $x\\prime$ are close to one another, the numerator approaches zero and $k(x,x\\prime) \\approx 1$, while when they are far apart the numerator becomes large and $k(x,x\\prime) \\approx 0$. The parameter $\\sigma$ controls how quickly an increased distance causes the value of the kernel to fall toward zero.\n",
      "\n",
      "If we associate a kernel with each point for a particular group that we are using as training examples, our classification function becomes:\n",
      "\n",
      "$$y = \\left\\{ \\begin{aligned} 1 &\\text{if } \\theta_0 + \\theta_1 k(x,x_1) + \\theta_2 k(x,x_2) + \\ldots \\ge 0\\\\\n",
      "0 &\\text{ otherwise}\\end{aligned}\\right.$$\n",
      "\n",
      "Consider particular values for the parameters, such as $\\theta_0=-0.5$ and $\\theta_i=1, \\, i=1,2,\\ldots$. This would result in the function evaluating to approximately 0.5 for a location that is close to any of the points in the set, and to -0.5 for locations that are reasonably far from all the points (as determined by the value of $\\sigma$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each feature $(x_i, y_i)$ in our dataset, we can calculate the similarity to each feature via the selected kernel:\n",
      "\n",
      "$$f_i = \\left[\\begin{align}\n",
      "k(x_i, &x_0) \\\\\n",
      "k(x_i, &x_1) \\\\\n",
      "k(x_i, &x_2) \\\\\n",
      "\\vdots & \\\\\n",
      "k(x_i, &x_n)\n",
      "\\end{align}\\right]$$\n",
      "\n",
      "notice that, under the Gaussian kernel at least, there will be one element $k(x_i, x_i)$ that evaluates to 1.\n",
      "\n",
      "To use the SVM, we use this $f \\in \\mathbb{R}^{n+1}$ to calculate the inner product $\\theta\\prime f$ and predict $y_i=1$ if $\\theta\\prime f_i \\ge 0$. We obtain the parameters for $\\theta$ by minimizing:\n",
      "\n",
      "$$\\min_{\\theta} \\left[ C \\sum_{i=1}^n y_i k_1(\\theta\\prime f_i) + (1-y_i) k_0(\\theta\\prime f_i) \\right] + \\frac{1}{2}\\sum_{j=1}^k \\theta^2_j$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There remains a choice to be made for the values of the SVM parameters. Recall $C$, which corresponds to the inverse of the regularization parameter in a lasso model. This choice of $C$ involves a bias-variance tradeoff:\n",
      "\n",
      "* large C = low bias, high variance\n",
      "* small C = high bias, low variance\n",
      "\n",
      "Similarly, if we are using the Gaussian kernel, we must choose a value for $\\sigma^2$. When $\\sigma^2$ is large, then features are considered similar over greater distances, resulting in a smoother decision boundary, while for smaller $\\sigma^2$, similarity falls off quickly with distance.\n",
      "\n",
      "* large $\\sigma^2$ = high bias, low variance\n",
      "* small $\\sigma^2$ = low bias, high variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Feature scaling\n",
      "\n",
      "It is important with many kernels to ***scale*** the features prior to using them in a SVM. This is because features which are numerically large relative to the others will tend to dominate the norm. So that each feature is able to contribute to the selection of the decision boundary, we want them all to have approximately the same range."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Linear kernel\n",
      "\n",
      "The simplest choice of kernel is to use no kernel at all, but rather to simply use the linear combination of the features themselves as the kernel. Hence,\n",
      "\n",
      "$$y = \\left\\{ \\begin{aligned} 1 &\\text{if } \\theta\\prime x \\ge 0\\\\\n",
      "0 &\\text{ otherwise}\\end{aligned}\\right.$$\n",
      "\n",
      "This approach is useful when there are a *large number of features*, but the *size of the dataset is small*. In this case, a simple linear decision boundary may be appropriate given that there is relatively little data. If the reverse is true, where there are a small number of features and plenty of data, a Gaussian kernel may be more appropriate, as it allows for a more complex decision boundary."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multi-class Classification\n",
      "\n",
      "In the exposition above, we have addressed binary classification problems. The SVM can be generalized to multi-class classification. This involves training $K$ binary classifiers, where each of $k=1,\\ldots,K$ classes is trained against the remaining classes pooled into a single group (\"all-versus-one\"). Then for each point, we select the class for which the inner product $\\theta_k\\prime x$ is largest."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVM using `scikit-learn`\n",
      "\n",
      "The scikit-learn machine learning package for Python includes a nice implementation of support vector machines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's begin with a fun enological example. Your textbook includes a dataset `wine.dat` that is the result of chemical analyses of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. (The response variable is incorrectly labeled `region`; it should be the grape from which the wine was derived). We might be able to correctly classify a given wine based on its chemical profile.\n",
      "\n",
      "To illustrate the characteristics of the SVM, we will select two attributes, which will make things easy to visualize."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wine = pd.read_table(\"data/wine.dat\", sep='\\s+')\n",
      "\n",
      "attributes = ['Alcohol',\n",
      "            'Malic acid',\n",
      "            'Ash',\n",
      "            'Alcalinity of ash',\n",
      "            'Magnesium',\n",
      "            'Total phenols',\n",
      "            'Flavanoids',\n",
      "            'Nonflavanoid phenols',\n",
      "            'Proanthocyanins',\n",
      "            'Color intensity',\n",
      "            'Hue',\n",
      "            'OD280/OD315 of diluted wines',\n",
      "            'Proline']\n",
      "\n",
      "grape = wine.pop('region')\n",
      "y = grape.values\n",
      "wine.columns = attributes\n",
      "X = wine[['Alcohol', 'Proline']].values\n",
      "\n",
      "svc = svm.SVC(kernel='linear',random_state=0)\n",
      "svc.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ?svm.SVC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wine.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is easiest to display the model fit graphically, by evaluating the model over a grid of points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.colors import ListedColormap\n",
      "# Create color maps for 3-class classification problem, as with iris\n",
      "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
      "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
      "\n",
      "def plot_estimator(estimator, X, y):\n",
      "    \n",
      "    try:\n",
      "        X, y = X.values, y.values\n",
      "    except AttributeError:\n",
      "        pass\n",
      "    \n",
      "    estimator.fit(X, y)\n",
      "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
      "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
      "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
      "                         np.linspace(y_min, y_max, 100))\n",
      "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    plt.figure()\n",
      "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
      "\n",
      "    # Plot also the training points\n",
      "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
      "    plt.axis('tight')\n",
      "    plt.axis('off')\n",
      "    plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_estimator(svc, X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SVM gets its name from the samples in the dataset from each class that lie closest to the other class. These training samples are called \"support vectors\" because changing their position in *p*-dimensional space would change the location of the decision boundary. \n",
      "\n",
      "In scikits-learn, the indices of the support vectors for each class can be found in the `support_vectors_` attribute of the `SVC` object. Here is a 2 class problem using only classes 1 and 2 in the wine dataset.\n",
      "\n",
      "The support vectors are circled."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract classes 1 and 2\n",
      "X, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])]\n",
      "\n",
      "    # ?np.in1d\n",
      "\n",
      "\n",
      "plot_estimator(svc, X, y)\n",
      "plt.scatter(svc.support_vectors_[:, 0], \n",
      "           svc.support_vectors_[:, 1], \n",
      "           s=120, \n",
      "           facecolors='none', \n",
      "           linewidths=2,\n",
      "           zorder=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, these classes are not linearly separable.\n",
      "\n",
      "As we learned, regularization is tuned via the $C$ parameter. In practice, a large $C$ value means that the number of support vectors is small, while a small $C$ implies many support vectors. scikit-learn sets a default value of $C=1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = svm.SVC(kernel='linear', C=1e6)\n",
      "plot_estimator(svc, X, y)\n",
      "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n",
      "            facecolors='none', linewidths=2, zorder=10)\n",
      "plt.title('High C values: small number of support vectors')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's do it again with a \"low\" $C$ value"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = svm.SVC(kernel='linear', C=1e-2)\n",
      "plot_estimator(svc, X, y)\n",
      "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n",
      "            facecolors='none', linewidths=2, zorder=10)\n",
      "plt.title('Low C values: high number of support vectors')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can choose from a suite of available kernels (`linear`, `poly`, `rbf`, `sigmoid`, `precomputed`) or a custom kernel can be passed as a function. Note that the radial basis function (`rbf`) kernel is just a Gaussian kernel, but with parameter $\\gamma=1/\\sigma^2$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Linear\n",
      "svc_lin = svm.SVC(kernel='linear')\n",
      "plot_estimator(svc_lin, X, y)\n",
      "plt.scatter(svc_lin.support_vectors_[:, 0], svc_lin.support_vectors_[:, 1], \n",
      "            s=80, facecolors='none', linewidths=2, zorder=10)\n",
      "plt.title('Linear kernel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Polynomial\n",
      "svc_poly = svm.SVC(kernel='poly', degree=3)\n",
      "plot_estimator(svc_poly, X, y)\n",
      "plt.scatter(svc_poly.support_vectors_[:, 0], svc_poly.support_vectors_[:, 1], \n",
      "           s=80, facecolors='none', linewidths=2, zorder=10)\n",
      "plt.title('Polynomial kernel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gausian (rbf)\n",
      "svc_rbf = svm.SVC(kernel='rbf', gamma=1e2)\n",
      "plot_estimator(svc_rbf, X, y)\n",
      "plt.scatter(svc_rbf.support_vectors_[:, 0], svc_rbf.support_vectors_[:, 1], \n",
      "           s=80, facecolors='none', linewidths=2, zorder=10)\n",
      "plt.title('RBF kernel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, the radial basis function (RBF) kernel is very flexible and performs best for this dataset. However, it is easy to get carried away tuning to a training dataset--we don't really believe the resulting decision boundary, do we?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cross-validation\n",
      "\n",
      "In order to make objective choices for either kernels or hyperparameter values, we can apply the cross-validation methods outlined in last week's lecture. Every estimator class in `scikit-learn` exposes a `score` method that can judge the quality of the fit (or the prediction) on new data.\n",
      "\n",
      "The `score(x,y)` method for the `SVC` class returns the mean accuracy of the predictions from `x` with respect to `y`, for the fitted SVM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_lin.fit(X[:100], y[:100])\n",
      "svc_lin.score(X[100:], y[100:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_poly.fit(X[:100], y[:100])\n",
      "svc_poly.score(X[100:], y[100:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_rbf.fit(X[:100], y[:100])\n",
      "svc_rbf.score(X[100:], y[100:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each estimator in `scikit-learn` has a default estimator score method, which is an evaluation criterion for the problem they are designed to solve. For the `SVC` class, this is the mean accuracy, as shown above.\n",
      "\n",
      "Alternately, if we use cross-validation, you can specify one of a set of built-in scoring metrics. For classifiers such as support vector machines, these include:\n",
      "\n",
      "* \"accuracy\":\t`sklearn.metrics.accuracy_score`\n",
      "* \"average_precision\":\t`sklearn.metrics.average_precision_score`\n",
      "* \"f1\":\t`sklearn.metrics.f1_score`\n",
      "* \"precision\":\t`sklearn.metrics.precision_score`\n",
      "* \"recall\":\t`sklearn.metrics.recall_score`\n",
      "* \"roc_auc\":\t`sklearn.metrics.roc_auc_score`\n",
      "\n",
      "Regression models can use appropriate metrics, like `mean_squared_error` or `r2`.\n",
      "\n",
      "Finally, one can specify arbitrary loss functions to be used for assessment. The `metrics` module implements functions assessing prediction errors for specific purposes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def custom_loss(observed, predicted):\n",
      "    diff = np.abs(observed - predicted).max()\n",
      "    return np.log(1 + diff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import make_scorer\n",
      "custom_scorer = make_scorer(custom_loss, greater_is_better=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implementing cross-validation on our wine SVC is straightforward:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
      "        wine.values, grape.values, test_size=0.4, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape, y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test.shape, y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = svm.SVC(kernel='linear', C=1)\n",
      "f.fit(X_train, y_train)\n",
      "f.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ?svm.SVC.score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the wine dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_validation.cross_val_score(f, wine.values, grape.values, cv=10)\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores_precision = cross_validation.cross_val_score(f, wine.values, grape.values, cv=5,\n",
      "                                 scoring='precision')\n",
      "scores_precision"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_precision.mean(), scores_precision.std() * 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The module `sklearn.metric` also exposes a set of simple functions measuring prediction error given observations and prediction, such as the confusion matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "svc_poly = svm.SVC(kernel='poly', degree=3).fit(X_train, y_train)\n",
      "confusion_matrix(y_test, svc_poly.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise: microbiome data\n",
      "\n",
      "Try to estimate a reasonable support vector classfier for the micribiome dataset (`../data/microbiome.csv`). Specifically, see if you can correctly classify the sample location (tissue or stool) from the counts of different bacterial taxa obtained from high-throughput RNA sequencing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import normalize\n",
      "\n",
      "microbiome = pd.read_csv(\"data/microbiome.csv\")\n",
      "\n",
      "microbiome_pivoted = microbiome.drop('Group', axis=1).pivot(index='Patient', \n",
      "                        columns='Taxon').stack(level=0).reset_index()\n",
      "microbiome_data = microbiome_pivoted.drop('Patient', \n",
      "                        axis=1).rename(columns={'level_1':'Location'}\n",
      "                                       ).replace({'Tissue': 0 , 'Stool':1})\n",
      "\n",
      "y = microbiome_data.values[:, 0]\n",
      "X = normalize(microbiome_data.values[:, 1:].astype(float))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "microbiome_data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "[Coursera's Machine Learning course](https://www.coursera.org/course/ml) by Stanford's Andrew Ng\n",
      "\n",
      "[`scikit-learn` User's Guide](http://scikit-learn.org/stable/modules/svm.html) SVM section\n",
      "\n",
      "[Scikit-learn tutorials for the Scipy 2013 conference](https://github.com/jakevdp/sklearn_scipy2013) by Jake Vanderplas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}